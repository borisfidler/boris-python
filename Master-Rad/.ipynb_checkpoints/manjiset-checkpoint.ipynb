{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#include libraries\n",
    "import csv # to open/close/append CSV\n",
    "import os # to check if file exists\n",
    "import nltk #natural language toolkit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from collections import Counter, defaultdict \n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exists, but failed to open.\n"
     ]
    }
   ],
   "source": [
    "file_exists = os.path.isfile('/Users/boris/Downloads/dice_com-job_us_sample 3.csv')\n",
    "\n",
    "\n",
    "\n",
    "# loop to check if file exists\n",
    "if file_exists == 0:\n",
    "    print('Error: dice_com-job_us_sample.csv does not exist!')\n",
    "elif file_exists == 1:\n",
    "    try:\n",
    "        CSV_file = pd.read_csv('/Users/boris/Downloads/dice_com-job_us_sample 3.csv ', sep=',', header='infer')\n",
    "        print('Exists, and file is ok.')\n",
    "    except Exception as e:\n",
    "        Date_Advertised = None\n",
    "        print('Exists, but failed to open.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Please enter a job to search for: ')\n",
    "userInput = \"oracle\" #input()#'account'\n",
    "print(userInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInput = str.lower(userInput)\n",
    "CSV_file2 = CSV_file.apply(lambda x: x.astype(str).str.lower())\n",
    "CSV_file2 = CSV_file2[CSV_file2['jobtitle'].str.contains(userInput)] #case sensitive copy only matching rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "CSV_file2['Job_Description_Without_Stopwords'] = CSV_file2['jobdescription'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "CSV_file2['Job_Skills_Without_Stopwords'] = CSV_file2['skills'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "#regular expresion in wich we swap all special charachters with a blank and all numbers with a blank also\n",
    "def clean_string(strings):\n",
    "    result = []\n",
    "    for value in strings:\n",
    "        value = value.strip()\n",
    "        value = re.sub('([!?\\',*+.$-/])',' ', value)\n",
    "        value = re.sub(\"\\d+\", \"\", value)\n",
    "        result.append(value)\n",
    "    return result\n",
    "\n",
    "# creates list where each document is an element\n",
    "CSV_file2['Job_Description_Without_Stopwords'] = clean_string(CSV_file2['Job_Description_Without_Stopwords'])\n",
    "CSV_file2['Job_Skills_Without_Stopwords'] = clean_string(CSV_file2['Job_Skills_Without_Stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = []\n",
    "for index, row in CSV_file2.iterrows():\n",
    "    bigram_list = bigram_list + [b for b in nltk.bigrams(row['Job_Description_Without_Stopwords'].split())]\n",
    "\n",
    "# Bi-gram Term Frequency =======================================================================================================================================================================\n",
    "bigram_tf = Counter(bigram_list) # list all bigram TF\n",
    "bigram_tf_df = pd.DataFrame.from_dict(bigram_tf, orient='index').reset_index() #turn class collections.Counter into Pandas DataFrame\n",
    "bigram_tf_df = bigram_tf_df.rename(columns={'index':'index2', 0:'count'}) #index is immutable tuple and will need to be changed to list/string and cleaned to allow combining with IDF dataframe\n",
    "\n",
    "index_list = []\n",
    "for index, row in bigram_tf_df.iterrows():\n",
    "    index_list.append(str(row[0][0]) + ' ' + str(row[0][1]))\n",
    "\n",
    "bigram_tf_df['index'] = index_list\n",
    "bigram_tf_df = bigram_tf_df.drop(['index2'], axis=1) # delete bigram tuple column\n",
    "bigram_tf_df = bigram_tf_df.sort_values(by='count', ascending=False) #sort based on vec_sum\n",
    "\n",
    "# Bi-gram Inverse-Document Frequency ===========================================================================================================================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(2,3), min_df = 0, max_df = 0.3, stop_words = 'english')\n",
    "TFIDF_terms = vectorizer.fit_transform(CSV_file2['Job_Description_Without_Stopwords']).toarray()\n",
    "TFIDF_df = pd.DataFrame(TFIDF_terms, columns=vectorizer.get_feature_names()) #pull terms into a dataframe\n",
    "IDF_df = TFIDF_df.T.reset_index() # rotate DataFrame\n",
    "IDF_df[IDF_df.columns[::-1]]\n",
    "IDF_df['vec_sum'] = IDF_df.sum(axis=1) #add column which is a sum of all other columns\n",
    "IDF_df = IDF_df.sort_values(by='vec_sum', ascending=False) #sort based on vec_sum\n",
    "IDF_TF = pd.DataFrame()\n",
    "IDF_TF = IDF_df[['index', 'vec_sum']].copy() #copy only the index and vector sum to the new dataframe\n",
    "\n",
    "# PRINT lists ==================================================================================================================================================================================\n",
    "print('\\nTop skills for your searched job based on Term Frequency are:\\n', bigram_tf_df.head(n=20)) # print dataframe\n",
    "\n",
    "print('\\n\\nTop skills for your searched job based on addition of Inverse-Document Frequency are: \\n', IDF_TF.head(n=20))# print only index and vec_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list = []\n",
    "for index, row in CSV_file2.iterrows():\n",
    "    bigram_list = bigram_list + [b for b in nltk.bigrams(row['Job_Skills_Without_Stopwords'].split())]\n",
    "\n",
    "# Bi-gram Term Frequency =======================================================================================================================================================================\n",
    "bigram_tf = Counter(bigram_list) # list all bigram TF\n",
    "bigram_tf_df = pd.DataFrame.from_dict(bigram_tf, orient='index').reset_index() #turn class collections.Counter into Pandas DataFrame\n",
    "bigram_tf_df = bigram_tf_df.rename(columns={'index':'index2', 0:'count'}) #index is immutable tuple and will need to be changed to list/string and cleaned to allow combining with IDF dataframe\n",
    "\n",
    "index_list = []\n",
    "for index, row in bigram_tf_df.iterrows():\n",
    "    index_list.append(str(row[0][0]) + ' ' + str(row[0][1]))\n",
    "\n",
    "bigram_tf_df['index'] = index_list\n",
    "bigram_tf_df = bigram_tf_df.drop(['index2'], axis=1) # delete bigram tuple column\n",
    "bigram_tf_df = bigram_tf_df.sort_values(by='count', ascending=False) #sort based on vec_sum\n",
    "\n",
    "# Bi-gram Inverse-Document Frequency ===========================================================================================================================================================\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(2,2), min_df = 0, stop_words = 'english')\n",
    "tfidf_matrix = vectorizer.fit_transform(CSV_file2['Job_Skills_Without_Stopwords']) #ovo sam dodao za testiranje\n",
    "TFIDF_terms = vectorizer.fit_transform(CSV_file2['Job_Skills_Without_Stopwords']).toarray()\n",
    "TFIDF_df = pd.DataFrame(TFIDF_terms, columns=vectorizer.get_feature_names()) #pull terms into a dataframe\n",
    "IDF_df = TFIDF_df.T.reset_index() # rotate DataFrame\n",
    "IDF_df[IDF_df.columns[::-1]]\n",
    "IDF_df['vec_sum'] = IDF_df.sum(axis=1) #add column which is a sum of all other columns\n",
    "IDF_df = IDF_df.sort_values(by='vec_sum', ascending=False) #sort based on vec_sum\n",
    "IDF_TF = pd.DataFrame()\n",
    "IDF_TF = IDF_df[['index', 'vec_sum']].copy() #copy only the index and vector sum to the new dataframe\n",
    "\n",
    "# PRINT lists ==================================================================================================================================================================================\n",
    "print('\\nTop skills for your searched job based on Term Frequency are:\\n', bigram_tf_df.head(n=20)) # print dataframe\n",
    "\n",
    "print('\\n\\nTop skills for your searched job based on addition of Inverse-Document Frequency are: \\n', IDF_TF.head(n=20))# print only index and vec_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_file2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypalette = sns.color_palette('GnBu_d', 40)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(y=CSV_file['skills'], order=CSV_file['skills'].value_counts().index, palette=mypalette)\n",
    "plt.ylabel('Job Title', fontsize=14)\n",
    "plt.xlabel('Number of Job postings', fontsize=14)\n",
    "plt.title(\"Most seeked Jobs\", fontsize=18)\n",
    "plt.ylim(20.5,-0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    " #Compute the cosine similarity matrix\n",
    "#cosine_sim = linear_kernel(events2, events2)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# http://scikit-learn.org/stable/modules/metrics.html#linear-kernel\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recommendations(title):\n",
    "    idx = indices[title]\n",
    "    #print (idx)\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    #print (sim_scores)\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    job_indices = [i[0] for i in sim_scores]\n",
    "    return titles.iloc[job_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_file2 = CSV_file2.reset_index()\n",
    "titles = CSV_file2['jobtitle']\n",
    "indices = pd.Series(CSV_file2.index, index=CSV_file['jobtitle'])\n",
    "#indices.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations(1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_file.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_file['skills2'] = CSV_file['jobtitle'] + CSV_file['skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_file['skills'] = CSV_file['skills'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2), stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(CSV_file['skills2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# http://scikit-learn.org/stable/modules/metrics.html#linear-kernel\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
